# Telco Customer Churn Prediction - PySpark Implementation

![PySpark](https://img.shields.io/badge/PySpark-3.4.0+-orange)
![MLflow](https://img.shields.io/badge/MLflow-2.0.0+-blue)
![Airflow](https://img.shields.io/badge/Airflow-2.7.0-red)
![Python](https://img.shields.io/badge/Python-3.8+-green)
![Pandas](https://img.shields.io/badge/Pandas-1.5.0+-purple)
![NumPy](https://img.shields.io/badge/NumPy-1.21.0+-blue)
![License](https://img.shields.io/badge/License-MIT-yellow)

## ğŸ“š Technology Stack

### Core Technologies
- **Apache Spark (PySpark 3.4.0+)**: Distributed data processing and ML
- **Apache Airflow (2.7.0)**: Workflow orchestration and scheduling
- **MLflow (2.0.0+)**: Experiment tracking and model registry
- **Python (3.8+)**: Primary programming language

### Data Processing & ML
- **PySpark ML**: Machine learning algorithms (GBT, Random Forest, Logistic Regression, Decision Tree)
- **Pandas (1.5.0+)**: Data manipulation and analysis
- **NumPy (1.21.0+)**: Numerical computing
- **PyArrow (5.0.0+)**: Efficient data transfer between Pandas and Spark

### Development & Deployment
- **Make**: Build automation and task execution
- **FastAPI (0.95.0+)**: API deployment (planned)
- **Uvicorn (0.20.0+)**: ASGI server
- **Delta Lake (2.0.0+)**: Data lake storage (optional)

### Monitoring & Visualization
- **Matplotlib (3.5.0+)**: Data visualization
- **Seaborn (0.11.0+)**: Statistical visualizations
- **Plotly (5.10.0+)**: Interactive plots
- **W&B (0.15.0+)**: Experiment tracking (optional)

## ğŸš€ Overview

A production-ready customer churn prediction system built with **PySpark ML** for distributed computing and scalable machine learning. This implementation provides enterprise-grade features including distributed data processing, **Apache Airflow** orchestration for workflow management, real-time inference, and comprehensive MLflow integration for experiment tracking and model registry.

### âœ¨ Key Features

- ğŸ”¥ **Distributed Computing**: Scale from single machine to cluster deployment with Apache Spark
- ğŸ¤– **Advanced ML Pipeline**: End-to-end PySpark ML with 4 algorithms (GBT, Random Forest, Logistic Regression, Decision Tree)
- ğŸ“Š **Real-time Streaming**: Live predictions with Structured Streaming
- ğŸ”„ **Airflow Orchestration**: Automated workflow management with 3 production DAGs (ML Pipeline, Hyperparameter Tuning, Model Monitoring)
- ğŸ“ˆ **MLflow Integration**: Experiment tracking, model registry, and artifact management
- ğŸ”§ **Makefile Automation**: Simplified setup and execution with `make` commands
- ğŸ› ï¸ **Production Ready**: Comprehensive logging, error handling, and monitoring
- ğŸ“¦ **Modular Architecture**: Separate modules for data ingestion, feature engineering, model training, and inference

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Data       â”‚â”€â”€â”€â–¶â”‚  PySpark Data    â”‚â”€â”€â”€â–¶â”‚  Processed Data â”‚
â”‚  (CSV/Parquet)  â”‚    â”‚  Pipeline        â”‚    â”‚  (Parquet)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         Apache Airflow Orchestration          â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚ ML Pipeline  â”‚  â”‚ Hyperparameter Tuningâ”‚  â”‚
        â”‚  â”‚     DAG      â”‚  â”‚        DAG           â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
        â”‚  â”‚    Model Monitoring DAG              â”‚    â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Registry â”‚â—€â”€â”€â”€â”‚  PySpark ML      â”‚â”€â”€â”€â–¶â”‚  Trained Models â”‚
â”‚  (MLflow)       â”‚    â”‚  Training        â”‚    â”‚  (Pipeline)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Predictions    â”‚â—€â”€â”€â”€â”‚  Streaming       â”‚â—€â”€â”€â”€â”‚  Live Data      â”‚
â”‚  (Real-time)    â”‚    â”‚  Inference       â”‚    â”‚  (JSON Stream)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **Note**: Kafka integration is planned for future releases to enable real-time data streaming.

## ğŸš€ Quick Start

### 1. Prerequisites

```bash
# Required
- Python 3.8+
- Java 8 or 11 (for PySpark)
- Minimum 4GB RAM (8GB+ recommended)

# Optional (for cluster deployment)
- Apache Spark 3.4.0+
- Apache Airflow 2.7.0 (installed via Makefile)
- Hadoop (for YARN deployment)
- Kubernetes (for K8s deployment)
```

### 2. Installation

```bash
# Clone repository
git clone https://github.com/vishwajayawickrama/Telco_Churn_Prediction_MLFlow_PySpark_AirFlow_Kafka.git
cd Telco_Churn_Prediction_MLFlow_PySpark_AirFlow_Kafka

# Install dependencies using Makefile
make install

# Activate virtual environment
source .venv/bin/activate
```

### 3. Setup Airflow (Optional for Workflow Orchestration)

```bash
# Setup local Airflow environment
make setup-local-airflow

# Start Airflow services
make airflow-start

# Access Airflow UI at http://localhost:8080
# Username: admin, Password: admin
```

### 4. Run Pipelines

#### Using Makefile (Recommended)

```bash
# Run complete ML pipeline
make run-all

# Or run individual pipelines
make data-pipeline        # Data preprocessing
make train-pipeline       # Model training
make streaming-inference  # Inference pipeline

# Start MLflow UI
make mlflow-ui           # Access at http://localhost:5001
```

#### Using Python Directly

```python
from pipelines.training_pipeline import training_pipeline

# Train model with PySpark
result = training_pipeline(
    data_path="./data/raw/TelcoCustomerChurnPrediction.csv",
    use_pyspark=True
)

print(f"Model accuracy: {result['evaluation_metrics']['accuracy']:.4f}")
```

## ğŸ“Š Usage Examples

### Airflow DAG Orchestration

The project includes three production-ready Airflow DAGs for automated workflow management:

#### 1. ML Pipeline DAG (`telco_churn_ml_pipeline_dag.py`)
Complete end-to-end ML pipeline orchestration:
```python
# Automated workflow includes:
# - Data preprocessing and feature engineering
# - Model training with hyperparameter tuning
# - Model evaluation and validation
# - Inference generation and result storage
# - Slack notifications for pipeline status
```

#### 2. Hyperparameter Tuning DAG (`telco_churn_hyperparameter_tuning_dag.py`)
Automated hyperparameter optimization:
```python
# Parallel tuning for multiple models:
# - Gradient Boosted Trees
# - Random Forest
# - Logistic Regression
# - Decision Tree
# - Best model selection and registration
```

#### 3. Model Monitoring DAG (`telco_churn_model_monitoring_dag.py`)
Production model monitoring and drift detection:
```python
# Continuous monitoring:
# - Model performance tracking
# - Data drift detection
# - Prediction quality monitoring
# - Automated retraining triggers
```

### Data Processing

```python
from pipelines.data_pipeline import data_pipeline

# Process data with PySpark
data_result = data_pipeline(
    data_path="./data/raw/TelcoCustomerChurnPrediction.csv",
    use_pyspark=True,
    force_rebuild=True
)

print(f"Training samples: {len(data_result['X_train'])}")
print(f"Test samples: {len(data_result['X_test'])}")
```

### Model Training & Comparison

```python
from pipelines.training_pipeline import compare_models_pyspark

# Compare multiple models
comparison = compare_models_pyspark(
    data_path="./data/raw/TelcoCustomerChurnPrediction.csv",
    model_types=['gbt', 'random_forest', 'logistic_regression'],
    test_size=0.2
)

# View results
for model, result in comparison['results'].items():
    if 'evaluation_metrics' in result:
        print(f"{model}: {result['evaluation_metrics']['accuracy']:.4f}")
```

### Real-time Inference

```python
from pipelines.streaming_inference_pipeline import streaming_inference

# Customer data
customer = {
    'gender': 'Female',
    'tenure': 24,
    'MonthlyCharges': 85.0,
    'PaymentMethod': 'Credit card (automatic)',
    # ... other features
}

# Get prediction
prediction = streaming_inference(
    use_pyspark=True,
    input_data=customer
)

print(f"Churn probability: {prediction['single_prediction']['churn_probability']:.4f}")
```

## ğŸ”§ Configuration

### Available Makefile Commands

```bash
# Installation and Setup
make install                 # Install dependencies and create virtual environment
make setup-local-airflow    # Initialize local Airflow environment
make clean                  # Clean up generated artifacts

# Pipeline Execution
make data-pipeline          # Run data preprocessing pipeline
make train-pipeline         # Run model training pipeline
make streaming-inference    # Run streaming inference pipeline
make run-all               # Execute all pipelines sequentially

# Airflow Operations
make airflow-start         # Start Airflow webserver and scheduler
make airflow-stop          # Stop Airflow services
make start-pipeline        # Start complete ML pipeline (Airflow + MLflow)

# MLflow Operations
make mlflow-ui            # Launch MLflow UI (default port: 5001)
make mlflow-clean         # Clean MLflow artifacts
make stop-all             # Stop all MLflow servers

# Development
make help                 # Display all available commands
```

### Spark Configuration (`config.yaml`)

```yaml
spark:
  app_name: "TelcoCustomerChurnPrediction"
  master: "local[*]"              # Use all cores
  executor_memory: "2g"           # Memory per executor
  driver_memory: "1g"             # Driver memory
  max_result_size: "1g"           # Max result size
  sql_adaptive_enabled: true      # Enable AQE
  sql_adaptive_coalesce_partitions_enabled: true
```

### MLflow Configuration

```yaml
mlflow:
  tracking_uri: "file:./mlruns"
  experiment_name: "telco_pyspark_experiment"
  autolog: true
  model_registry_name: "pyspark_churn_model"
  run_name_prefix: "pyspark_run"
```

### Model Configuration

```yaml
model:
  # PySpark ML algorithms supported
  model_type: "gbt_classifier"  # Options: gbt_classifier, random_forest_classifier, 
                                 #          logistic_regression, decision_tree_classifier
  model_types:
    gbt_classifier:              # Gradient Boosted Trees
      max_iter: 100
      max_depth: 10
      step_size: 0.1
      seed: 42
    random_forest_classifier:
      num_trees: 100
      max_depth: 10
      seed: 42
      feature_subset_strategy: "auto"
    logistic_regression:
      max_iter: 1000
      reg_param: 0.01
      elastic_net_param: 0.0
      family: "binomial"
    decision_tree_classifier:
      max_depth: 10
      seed: 42
      impurity: "gini"
```

### Airflow Configuration

Airflow DAGs are configured with:
- **Default Owner**: data-science-team
- **Retries**: 2 with 5-minute delay
- **Execution Timeout**: 4 hours
- **Email Notifications**: Enabled for failures
- **Schedule**: Configurable per DAG (default: daily)

## ğŸš€ Deployment Options

### Local Development

```bash
# Single machine with all cores (using Makefile)
make run-all

# Or with Airflow orchestration
make start-pipeline
# Access Airflow UI: http://localhost:8080
# Access MLflow UI: http://localhost:5001
```

### Standalone Spark Cluster

```bash
# Start Spark cluster
$SPARK_HOME/sbin/start-master.sh
$SPARK_HOME/sbin/start-workers.sh

# Submit job
spark-submit \
  --master spark://master:7077 \
  --executor-memory 4g \
  --driver-memory 2g \
  pipelines/training_pipeline.py
```

### YARN Cluster

```bash
# Submit to YARN
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 4g \
  --num-executors 10 \
  pipelines/training_pipeline_pyspark.py
```

### Cloud Deployment

#### AWS EMR
```bash
aws emr create-cluster \
  --name "Telco-Churn-PySpark" \
  --release-label emr-6.4.0 \
  --applications Name=Spark \
  --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m5.xlarge
```

#### Azure HDInsight
```bash
az hdinsight create \
  --name telco-churn-cluster \
  --type spark \
  --cluster-login-password myPassword
```

#### Google Cloud Dataproc
```bash
gcloud dataproc clusters create telco-churn-cluster \
  --enable-autoscaling \
  --max-workers 10
```

## ğŸ“ˆ Performance Optimization

### Memory Tuning

| Dataset Size | Executor Memory | Driver Memory | Max Result Size |
|--------------|----------------|---------------|-----------------|
| < 1GB        | 1g             | 512m          | 512m            |
| 1-10GB       | 4g             | 2g            | 2g              |
| 10-100GB     | 8g             | 4g            | 4g              |
| > 100GB      | 16g            | 8g            | 8g              |

### Best Practices

1. **Cache DataFrames**: Cache frequently accessed data
2. **Use Parquet**: Store data in columnar format
3. **Optimize Partitions**: Balance parallelism vs overhead
4. **Broadcast Joins**: Use for small lookup tables
5. **Enable AQE**: Adaptive Query Execution for optimization

## ğŸ§ª Testing

### Run All Pipelines

```bash
# Using Makefile (Recommended)
make run-all

# Individual pipelines
make data-pipeline
make train-pipeline
make streaming-inference
```

### Test Individual Components

```python
# Test data pipeline
from pipelines.data_pipeline import data_pipeline
result = data_pipeline(
    data_path="./data/raw/TelcoCustomerChurnPrediction.csv",
    use_pyspark=True,
    force_rebuild=True
)

# Test model training
from pipelines.training_pipeline import training_pipeline
model = training_pipeline(
    model_type='gbt_classifier',
    use_pyspark=True
)

# Test inference
from pipelines.streaming_inference_pipeline import streaming_inference
prediction = streaming_inference(
    use_pyspark=True,
    input_data=sample_customer
)
```

### Test Airflow DAGs

```bash
# Test DAG syntax
airflow dags list

# Test specific DAG
airflow dags test telco_churn_ml_pipeline_dag 2024-01-01

# Trigger DAG manually
airflow dags trigger telco_churn_ml_pipeline_dag
```

## ğŸ“Š Monitoring & Observability

### Airflow UI
- **Local**: http://localhost:8080
- **Credentials**: admin / admin
- Monitor DAG runs, task status, and execution logs
- View task dependencies and execution history

### Spark UI
- **Local**: http://localhost:4040
- **Cluster**: Check cluster manager UI
- Monitor job execution, stages, and tasks
- View executor metrics and storage

### MLflow UI
```bash
# Start MLflow server
make mlflow-ui

# Access at http://localhost:5001
```
- Track experiments and runs
- Compare model performance
- View artifacts and parameters
- Manage model registry

### Logging
All components include comprehensive logging:
- **Data Pipeline**: Processing steps and statistics
- **Training**: Model metrics and performance
- **Inference**: Prediction results and timing
- **Errors**: Detailed error tracking and recovery

## ğŸ” Troubleshooting

### Common Issues

#### Memory Errors
```bash
# Increase memory allocation
export SPARK_DRIVER_MEMORY=4g
export SPARK_EXECUTOR_MEMORY=8g
```

#### Java Issues
```bash
# Set Java home
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

#### Serialization Errors
Ensure all functions used in Spark transformations are serializable.

#### Performance Issues
- Check Spark UI for job details
- Optimize partition sizes
- Enable adaptive query execution

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ Readme.md                    # Project documentation
â”œâ”€â”€ Makefile                     # Automation commands for setup and execution
â”œâ”€â”€ config.yaml                  # Central configuration file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚
â”œâ”€â”€ dags/                        # Apache Airflow DAGs
â”‚   â”œâ”€â”€ telco_churn_ml_pipeline_dag.py              # Main ML pipeline orchestration
â”‚   â”œâ”€â”€ telco_churn_hyperparameter_tuning_dag.py    # Hyperparameter optimization
â”‚   â””â”€â”€ telco_churn_model_monitoring_dag.py         # Model monitoring and drift detection
â”‚
â”œâ”€â”€ pipelines/                   # End-to-end pipeline implementations
â”‚   â”œâ”€â”€ data_pipeline.py         # Data ingestion and preprocessing pipeline
â”‚   â”œâ”€â”€ training_pipeline.py     # Model training and evaluation pipeline
â”‚   â””â”€â”€ streaming_inference_pipeline.py  # Real-time inference pipeline
â”‚
â”œâ”€â”€ src/                         # Core PySpark ML modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_ingestion.py        # PySpark data loading and validation
â”‚   â”œâ”€â”€ handle_missing_values.py # Missing value imputation strategies
â”‚   â”œâ”€â”€ outlier_detection.py     # Outlier detection and handling
â”‚   â”œâ”€â”€ feature_binning.py       # Feature binning transformations
â”‚   â”œâ”€â”€ feature_encoding.py      # PySpark ML encoders (StringIndexer, OneHotEncoder)
â”‚   â”œâ”€â”€ feature_scaling.py       # Feature scaling (StandardScaler, MinMaxScaler)
â”‚   â”œâ”€â”€ data_spiltter.py         # Train-test splitting
â”‚   â”œâ”€â”€ model_building.py        # PySpark ML model builders
â”‚   â”œâ”€â”€ model_training.py        # Training orchestration with cross-validation
â”‚   â”œâ”€â”€ model_evaluation.py      # Model evaluation metrics
â”‚   â””â”€â”€ model_inference.py       # Batch and streaming inference
â”‚
â”œâ”€â”€ utils/                       # Utility modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ spark_utils.py           # Spark session management and configuration
â”‚   â”œâ”€â”€ mlflow_utils.py          # MLflow integration for PySpark
â”‚   â”œâ”€â”€ airflow_utils.py         # Airflow helper functions
â”‚   â”œâ”€â”€ config.py                # Configuration management
â”‚   â””â”€â”€ logger.py                # Logging configuration
â”‚
â””â”€â”€ artifacts/                   # Generated artifacts (gitignored)
    â”œâ”€â”€ models/                  # Trained PySpark ML pipelines
    â”œâ”€â”€ data/                    # Processed data (Parquet format)
    â”œâ”€â”€ evaluation/              # Model evaluation reports
    â””â”€â”€ mlflow_training_artifacts/  # MLflow experiment artifacts

Note: data/ directory structure is created at runtime when pipelines are executed.
      Raw data should be placed in data/raw/ directory.
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Apache Spark** for distributed computing framework
- **Apache Airflow** for workflow orchestration and scheduling
- **MLflow** for experiment tracking and model registry
- **PySpark ML** for scalable machine learning capabilities
- **Community** for continuous support and contributions

## ğŸš§ Future Enhancements

- [ ] **Kafka Integration**: Real-time data streaming from Kafka topics
- [ ] **REST API**: FastAPI-based prediction service
- [ ] **Docker Support**: Containerized deployment
- [ ] **Kubernetes**: Cloud-native orchestration
- [ ] **Advanced Monitoring**: Prometheus and Grafana integration
- [ ] **A/B Testing**: Model comparison in production
- [ ] **Feature Store**: Centralized feature management

---

## ğŸ“ Support

For questions, issues, or contributions:

- **Issues**: Open an issue on GitHub
- **Discussions**: Use GitHub Discussions for questions
- **Documentation**: See inline code documentation and config.yaml

---

**ğŸš€ Built for scalable, production-ready machine learning with PySpark, Airflow, and MLflow!**

*Developed with â¤ï¸ for enterprise-scale machine learning and automated workflow orchestration*