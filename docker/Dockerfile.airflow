# Dockerfile for Airflow with PySpark and MLflow Integration

FROM apache/airflow:2.7.3-python3.9

# Switch to root to install system dependencies
USER root

# Install system dependencies for PySpark and Java
RUN apt-get update && \
    apt-get install -y \
    openjdk-11-jdk \
    wget \
    curl \
    procps \
    && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH=$PATH:$JAVA_HOME/bin

# Install Spark
ENV SPARK_VERSION=3.4.1
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV PYSPARK_PYTHON=python3

RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} $SPARK_HOME && \
    chown -R airflow:root $SPARK_HOME

# Switch back to airflow user
USER airflow

# Install Python dependencies
COPY requirements.airflow.txt /tmp/requirements.txt
RUN pip install --no-cache-dir -r /tmp/requirements.txt

# Install additional ML and data processing packages
RUN pip install --no-cache-dir \
    pyspark==3.4.1 \
    mlflow==2.8.1 \
    scikit-learn==1.3.2 \
    xgboost==2.0.2 \
    pandas==2.1.4 \
    numpy==1.24.4 \
    matplotlib==3.8.2 \
    seaborn==0.13.0 \
    plotly==5.17.0 \
    joblib==1.3.2 \
    psycopg2-binary==2.9.9 \
    sqlalchemy==1.4.48 \
    pymongo==4.6.0 \
    boto3==1.34.0 \
    azure-storage-blob==12.19.0 \
    google-cloud-storage==2.10.0

# Set environment variables for Airflow
ENV AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
ENV AIRFLOW__CORE__LOAD_EXAMPLES=false
ENV AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
ENV AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=30
ENV AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT=false
ENV AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true
ENV AIRFLOW__CORE__ENABLE_XCOM_PICKLING=true

# Create necessary directories
USER root
RUN mkdir -p /opt/airflow/dags /opt/airflow/logs /opt/airflow/plugins /opt/airflow/artifacts /opt/airflow/data && \
    chown -R airflow:root /opt/airflow

USER airflow

# Copy project files (will be mounted in docker-compose)
WORKDIR /opt/airflow